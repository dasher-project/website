HiddenPage Making the Training Text
<h2>Making the Training Text</h2>
<p> The training text should be a plain text file containing
 text `similar' to what you intend to write.
 The larger the better.
 We think that 300K is a good size to aim for.
 Our preferred <b>file format</b> for all corpuses is <b>UTF-8</b>, but
 if you prefer to provide another format, that's OK; when supplying the file
 please include a description of its contents and name the format; we'll
 use the linux utility <b>iconv</b> to convert, if necessary.
</p><p>
 Example training texts that you could use are:
<ul>
  <li>Take all the documents you have written, and glue them
       all together in one big document. 
</li>
  <li>Use novels - eg, we used Jane Austen's <em>Emma</em> from
   <a href=http://promo.net/pg/>Project Gutenberg</a>. The problem with using
       just one or two novels, however, is that particular words (like Emma or Alice)
       occur very frequently; so novels are not ideal for a general-purpose training text.
</li>
       <li>Use all the email messages you have written, and glue them
       all together in one big document..
</li>
     </ul>

     <h3>Existing corpuses</h3>
<a href=http://www.hit.uib.no/corpora/>There is a corpora mailing list; this website has lots of
     useful links</a>.


<h3>How to make a general-purpose training text</h3>     
<p>
 You can make a pretty good corpus simply by concatenating
 a load of documents in your chosen language. Such a corpus is
 pretty good, but not ideal, since, for example, if you
 include all of <em>Alice in Wonderland</em>, the word <em>Alice</em>
 and the phrase <em>white rabbit</em> will
 occur far more often than normal. The aims
 of the more complicated procedure described below are
<ol>
  <li>to create a corpus that has all common
       words represented in a variety of contexts, with no
       one source document dominating the statistics;
       </li>
  <li>to create a corpus that can be sensibly
       shrunk to make a smaller corpus (for handheld computers
       with small memory, for example).
       </li>
</ol>
</p><p>
 Here's how I made the training text for the English version of
 Dasher.
</p>
<ol>
  <li>
       Get lots of English documents.  Get far more material than you think you need,
       so that we can select a <em>well-balanced</em>
       set of sentences in a sensible way, as follows.
       </li>
  <li>
       Pre-process them all so that there is exactly one sentence per line.
<br>       I did this using a perl program I wrote,
       <a href=/mackay/perl/processbook.p>processbook.p</a>
       with scripts like this
       <pre>
foreach f ( alice emma )
  processbook.p  /books0/$f > /books/$f
end
</pre>       
       </li>
  <li><b>Now</b>, obtain a listing of the 2000 most frequent words in
       the language. The idea is, since these words are common, it is important that we should
       have them represented several times each in the final corpus, in a variety of
       contexts. We will use these words to select which sentences are included
       from our over-large corpus.
       <br>
       I obtained such a list from the internet and put it in a file called <tt>dict</tt>.
       I removed from dict any absurdly common words that prevented the remaining steps from
       working nicely.
       </li>
  <li>
       Use another  program to select from each pre-processed book the sentences
       that contain the 2000 required words. Go through the required words in order,
       so that the resulting corpus is also ordered, with the top of the corpus containing
       examples of use of the most common words; that way, the corpus can be shrunk by cutting
       its tail off, and should still be an appropriate corpus for its size.
       <br>
       Glue the sentences together into plausible-sized paragraphs, so as to emulate
       normal writing.
       <br>
       I did this step by using the linux utility <b>glimpse</b> and my perl
       program <a href=/mackay/perl/corpus.p>corpus.p</a>
       <pre>
rm  /data/coll/mackay/books/*~
glimpseindex -b  -B   -H ~/dasher/  /data/coll/mackay/books/
corpus.p k=1 f=4 o=corpus4.txt
       </pre>
       That's how I made <a href=/dasher/download/english/corpus4.txt1>this corpus</a> (316K),
       which is used in Dasher 1.6.8.
       </li>
       <li>
       If you have any non-ASCII characters, you need to convert the file to 
       UTF-8 (or send it to us so we can do it). iconv is a Unix tool that can
       do this. If your text is in ISO-8859-1 format (ie, Western Europe), run
<pre>
iconv -f iso-8859-1 -t utf-8 corpus >corpus.utf8
</pre>
which will produce a UTF-8 version of the corpus in corpus.utf8.
	    Another method for converting to UTF8 using perl is this:
<pre>
#!/usr/local/bin/perl5.8.4
use Encode;

local $/;
open(TEXT, "&lt; infile.txt") or die $!;
open (UTF8, "> outfile.txt") or die $!;
my $data = &lt;TEXT>;
Encode::from_to($data, "iso-8859-1", "utf8"); # the converting line
binmode UTF8; # the filehandle should be binary for printing UTF8
print UTF8 $data;
	    </pre>
	    A useful command for checking what format a file is in is
	    <pre>file filename.txt
	    </pre>
</li>
</ol>

</p><p>
 If people make good corpuses in other languages and wish to share
 them, I can put them on <a href=/dasher/download/training>this site</a>.
</p>
<p>
 Two-letter country codes
 for the ends of training filenames
 can be found at <a href=http://www.theodora.com/country_digraphs.html>(digraphs page)</a>.
</p>

